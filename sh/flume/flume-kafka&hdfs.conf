## Explain
#通过sink把数据分别输出到kafka和HDFS上


# Name the components on this a1
a1.sources = r1
a1.sinks = k1 k2
a1.channels = c1 c2

# Describe/configuration the source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /data/flume-1.9.0/tail_dir.json
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.filegroups.f1 = /flume-logs/.*file.*
a1.sources.r1.filegroups.f2 = /flume-logs/.*log.*
#忽略所有以.tmp结尾的文件，不上传
a1.sources.r1.ignorePattern = ([^ ]*\.tmp)

## kafka

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
#Describe the sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.topic = kafkatest
a1.sinks.k1.kafka.bootstrap.servers = bigdata-02:6667,bigdata-03:6667,bigdata-04:6667
a1.sinks.k1.kafka.flumeBatchSize = 2000
a1.sinks.k1.kafka.producer.acks = 1
a1.sinks.k1.channel = c1


## hdfs

## Use a channel which buffers events in memory
a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100
#Describe the sink
a1.sinks.k2.type = hdfs
a1.sinks.k2.hdfs.path = hdfs://bigdata-master:8020/flume/upload/%Y%m%d/%H
#上传文件的前缀
#a3.sinks.k3.hdfs.filePrefix = log-
a1.sinks.k2.hdfs.round = true
a1.sinks.k2.hdfs.roundValue = 1
a1.sinks.k2.hdfs.roundUnit = hour
a1.sinks.k2.hdfs.useLocalTimeStamp = true
#积攒多少个Event才flush到HDFS一次
a1.sinks.k2.hdfs.batchSize = 100
a1.sinks.k2.hdfs.fileType = DataStream
a1.sinks.k2.hdfs.rollInterval = 60
#设置每个文件的滚动大小大概是128M
a1.sinks.k2.hdfs.rollSize = 134217700
a1.sinks.k2.hdfs.rollCount = 0

# Bind the source and sink to the channel
a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1

## Bind the source and sink to the channel
a1.sinks.k2.channel = c2